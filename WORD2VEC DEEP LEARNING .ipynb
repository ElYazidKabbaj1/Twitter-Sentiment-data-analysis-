{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82dac56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a45df688",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "208aa01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\SETUP\n",
      "[nltk_data]     GAME\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\SETUP GAME\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\SETUP\n",
      "[nltk_data]     GAME\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31ca50d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f4b0237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f643539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec #Word2Vec is mostly used for huge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7624653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43943, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "df_train=pd.read_csv(r\"twitter_sentiment_data.csv\")\n",
    "print(df_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1861ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1    22962\n",
      " 2     9276\n",
      " 0     7715\n",
      "-1     3990\n",
      "Name: sentiment, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SETUP GAME\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='sentiment'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CLASS DISTRIBUTION\n",
    "#if dataset is balanced or not\n",
    "x=df_train['sentiment'].value_counts()\n",
    "print(x)\n",
    "sns.barplot(x.index,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bf921bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    0\n",
       "message      0\n",
       "tweetid      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Missing values\n",
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fb20b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. WORD-COUNT\n",
    "df_train['word_count'] = df_train['message'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "#2. CHARACTER-COUNT\n",
    "df_train['char_count'] = df_train['message'].apply(lambda x: len(str(x)))\n",
    "\n",
    "#3. UNIQUE WORD-COUNT\n",
    "df_train['unique_word_count'] = df_train['message'].apply(lambda x: len(set(str(x).split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e93ec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Common text preprocessing\n",
    "text = \"   This is a message to be cleaned. It may involve some things like: <br>, ?, :, ''  adjacent spaces and tabs     .  \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca3629d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to lowercase and remove punctuations and characters and then strip\n",
    "def preprocess(text):\n",
    "    text = text.lower() #lowercase text\n",
    "    text=text.strip()  #get rid of leading/trailing whitespace \n",
    "    text=re.compile('<.*?>').sub('', text) #Remove HTML tags/markups\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  #Replace punctuation with space. Careful since punctuation can sometime be useful\n",
    "    text = re.sub('\\s+', ' ', text)  #Remove extra space and tabs\n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) #[0-9] matches any digit (0 to 10000...)\n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) #matches any digit from 0 to 100000..., \\D matches non-digits\n",
    "    text = re.sub(r'\\s+',' ',text) #\\s matches any whitespace, \\s+ matches multiple whitespace, \\S matches non-whitespace \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92908309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a message to be cleaned it may involve some things like adjacent spaces and tabs\n"
     ]
    }
   ],
   "source": [
    "text=preprocess(text)\n",
    "print(text)  #text is a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "269fa371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message cleaned may involve things like adjacent spaces tabs\n",
      "messag clean may involv thing like adjac space tab\n",
      "messag clean may involv thing like adjac space tab\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>@tiniebeany climate change is an interesting h...</td>\n",
       "      <td>792927353886371840</td>\n",
       "      <td>tiniebeany climate change interest hustle glob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @NatGeoChannel: Watch #BeforeTheFlood right...</td>\n",
       "      <td>793124211518832641</td>\n",
       "      <td>rt natgeochannel watch beforetheflood right le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Fabulous! Leonardo #DiCaprio's film on #climat...</td>\n",
       "      <td>793124402388832256</td>\n",
       "      <td>fabulous leonardo dicaprio film climate change...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @Mick_Fanning: Just watched this amazing do...</td>\n",
       "      <td>793124635873275904</td>\n",
       "      <td>rt mick fan watch amazing documentary leonardo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @cnalive: Pranita Biswasi, a Lutheran from ...</td>\n",
       "      <td>793125156185137153</td>\n",
       "      <td>rt cnalive pranita biswasi lutheran odisha giv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  \\\n",
       "0         -1  @tiniebeany climate change is an interesting h...   \n",
       "1          1  RT @NatGeoChannel: Watch #BeforeTheFlood right...   \n",
       "2          1  Fabulous! Leonardo #DiCaprio's film on #climat...   \n",
       "3          1  RT @Mick_Fanning: Just watched this amazing do...   \n",
       "4          2  RT @cnalive: Pranita Biswasi, a Lutheran from ...   \n",
       "\n",
       "              tweetid                                         clean_text  \n",
       "0  792927353886371840  tiniebeany climate change interest hustle glob...  \n",
       "1  793124211518832641  rt natgeochannel watch beforetheflood right le...  \n",
       "2  793124402388832256  fabulous leonardo dicaprio film climate change...  \n",
       "3  793124635873275904  rt mick fan watch amazing documentary leonardo...  \n",
       "4  793125156185137153  rt cnalive pranita biswasi lutheran odisha giv...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3. LEXICON-BASED TEXT PROCESSING EXAMPLES\n",
    " \n",
    "#1. STOPWORD REMOVAL\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)\n",
    "\n",
    "text=stopword(text)\n",
    "print(text)\n",
    "\n",
    "#2. STEMMING\n",
    " \n",
    "# Initialize the stemmer\n",
    "snow = SnowballStemmer('english')\n",
    "def stemming(string):\n",
    "    a=[snow.stem(i) for i in word_tokenize(string) ]\n",
    "    return \" \".join(a)\n",
    "text=stemming(text)\n",
    "print(text)\n",
    "\n",
    "#3. LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    " \n",
    "# This is a helper function to map NTLK position tags\n",
    "# Full list is available here: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)\n",
    "\n",
    "text = lemmatizer(text)\n",
    "print(text)\n",
    "\n",
    "\n",
    "#FINAL PREPROCESSING\n",
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))\n",
    "\n",
    "df_train['clean_text'] = df_train['message'].apply(lambda x: finalpreprocess(x))\n",
    "df_train=df_train.drop(columns=['word_count','char_count','unique_word_count'])\n",
    "df_train.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82e990cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Word2vec model\n",
    "#here words_f should be a list containing words from each document. say 1st row of the list is words from the 1st document/sentence\n",
    "#length of words_f is number of documents/sentences in your dataset\n",
    "df_train['clean_text_tok']=[nltk.word_tokenize(i) for i in df_train['clean_text']] #convert preprocessed sentence to tokenized sentence\n",
    "model = Word2Vec(df_train['clean_text_tok'],min_count=1)  #min_count=1 means word should be present at least across all documents,\n",
    "#if min_count=2 means if the word is present less than 2 times across all the documents then we shouldn't consider it\n",
    "\n",
    "#w2v = dict((model.wv.key_to_index.keys()))\n",
    "w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))  #combination of word and its vector\n",
    "#for converting sentence to vectors/numbers from word vectors result by Word2Vec\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7311568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLITTING THE TRAINING DATASET INTO TRAINING AND VALIDATION\n",
    " \n",
    "X_train, X_val, y_train, y_val = train_test_split(df_train[\"clean_text\"],\n",
    "                                                  df_train[\"sentiment\"],\n",
    "                                                  test_size=0.2,\n",
    "                                                  shuffle=True)\n",
    "X_train_tok= [nltk.word_tokenize(i) for i in X_train]  #for word2vec\n",
    "X_val_tok= [nltk.word_tokenize(i) for i in X_val]      #for word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca68af3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.67      0.10      0.17       803\n",
      "           0       0.56      0.30      0.39      1553\n",
      "           1       0.64      0.87      0.74      4540\n",
      "           2       0.64      0.58      0.61      1893\n",
      "\n",
      "    accuracy                           0.64      8789\n",
      "   macro avg       0.63      0.46      0.48      8789\n",
      "weighted avg       0.63      0.64      0.60      8789\n",
      "\n",
      "Confusion Matrix: [[  77  138  507   81]\n",
      " [  16  468  911  158]\n",
      " [  19  199 3944  378]\n",
      " [   3   34  753 1103]]\n"
     ]
    }
   ],
   "source": [
    "#Word2vec\n",
    "# Fit and transform\n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "X_train_vectors_w2v = modelw.transform(X_train_tok)\n",
    "X_val_vectors_w2v = modelw.transform(X_val_tok)\n",
    "\n",
    "#FITTING THE CLASSIFICATION MODEL using Logistic Regression (W2v)\n",
    "lr_w2v=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "lr_w2v.fit(X_train_vectors_w2v, y_train)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = lr_w2v.predict(X_val_vectors_w2v)\n",
    "y_prob = lr_w2v.predict_proba(X_val_vectors_w2v)[:,1]\n",
    " \n",
    "\n",
    "print(classification_report(y_val,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_val, y_predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cf14dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
