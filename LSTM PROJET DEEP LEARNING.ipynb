{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "606554ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd    # to load dataset\n",
    "import numpy as np     # for mathematic equation\n",
    "from nltk.corpus import stopwords   # to get collection of stopwords\n",
    "from sklearn.model_selection import train_test_split       # for splitting dataset\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating\n",
    "from tensorflow.keras.models import Sequential     # the model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense # layers of the architecture\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n",
    "from tensorflow.keras.models import load_model   # load saved model\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af825002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       sentiment                                            message  \\\n",
      "0             -1  @tiniebeany climate change is an interesting h...   \n",
      "1              1  RT @NatGeoChannel: Watch #BeforeTheFlood right...   \n",
      "2              1  Fabulous! Leonardo #DiCaprio's film on #climat...   \n",
      "3              1  RT @Mick_Fanning: Just watched this amazing do...   \n",
      "4              2  RT @cnalive: Pranita Biswasi, a Lutheran from ...   \n",
      "...          ...                                                ...   \n",
      "43938          1  Dear @realDonaldTrump,\\nYeah right. Human Medi...   \n",
      "43939          1  What will your respective parties do to preven...   \n",
      "43940          2  RT @MikkiL: UN Poll Shows Climate Change Is th...   \n",
      "43941          0  RT @taehbeingextra: i still can$q$t believe th...   \n",
      "43942          1  @Likeabat77 @zachhaller \\n\\nThe wealthy + foss...   \n",
      "\n",
      "                  tweetid  \n",
      "0      792927353886371840  \n",
      "1      793124211518832641  \n",
      "2      793124402388832256  \n",
      "3      793124635873275904  \n",
      "4      793125156185137153  \n",
      "...                   ...  \n",
      "43938  791307031919550464  \n",
      "43939  791316857403936768  \n",
      "43940  791357509101621249  \n",
      "43941  791390042136641537  \n",
      "43942  791401610308038656  \n",
      "\n",
      "[43943 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"twitter_sentiment_data.csv\")\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dca1f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#StopWords\n",
    "english_stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd0d66c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message\n",
      "0        [tiniebeany, climate, change, interesting, hus...\n",
      "1        [rt, natgeochannel, watch, beforetheflood, rig...\n",
      "2        [fabulous, leonardo, dicaprio, film, climate, ...\n",
      "3        [rt, mick, fanning, just, watched, amazing, do...\n",
      "4        [rt, cnalive, pranita, biswasi, lutheran, odis...\n",
      "                               ...                        \n",
      "43938    [dear, realdonaldtrump, yeah, right, human, me...\n",
      "43939    [what, respective, parties, prevent, climate, ...\n",
      "43940    [rt, mikkil, un, poll, shows, climate, change,...\n",
      "43941    [rt, taehbeingextra, still, q, believe, gif, t...\n",
      "43942    [likeabat, zachhaller, the, wealthy, fossil, f...\n",
      "Name: message, Length: 43943, dtype: object \n",
      "\n",
      "Sentiment\n",
      "0        3\n",
      "1        1\n",
      "2        1\n",
      "3        1\n",
      "4        2\n",
      "        ..\n",
      "43938    1\n",
      "43939    1\n",
      "43940    2\n",
      "43941    0\n",
      "43942    1\n",
      "Name: sentiment, Length: 43943, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Load and clean Dataset \n",
    "#Encode Sentiments \n",
    "def load_dataset():\n",
    "    df = pd.read_csv(r\"twitter_sentiment_data.csv\")\n",
    "    x_data = df['message']       # message/Input\n",
    "    y_data = df['sentiment']    # Sentiment/Output\n",
    "\n",
    "    # PRE-PROCESS REVIEW\n",
    "    x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
    "    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n",
    "    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  # remove stop words\n",
    "    x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n",
    "    \n",
    "    # ENCODE SENTIMENT -> 0 & 1\n",
    "    y_data = y_data.replace(0, 0)\n",
    "    y_data = y_data.replace(1, 1)\n",
    "    y_data = y_data.replace(2, 2)\n",
    "    y_data = y_data.replace(-1, 3)\n",
    "    return x_data, y_data\n",
    "\n",
    "x_data, y_data = load_dataset()\n",
    "\n",
    "print('Message')\n",
    "print(x_data, '\\n')\n",
    "print('Sentiment')\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab06e169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set\n",
      "27202    [rt, enviro, voter, in, hrs, maria, went, cat,...\n",
      "27644    [rt, allsoulkind, christiebeaches, johnnyfr, t...\n",
      "28618    [rt, karen, douglas, our, chapter, climate, ch...\n",
      "35219    [rt, usatoday, military, leaders, sounding, an...\n",
      "5766     [rt, katrinanation, trump, denial, catastrophi...\n",
      "                               ...                        \n",
      "22585    [rt, time, justin, trudeau, kayaked, family, t...\n",
      "41367    [if, treated, terrorism, urgency, considering,...\n",
      "43394    [rt, katz, clinton, donald, thinks, climate, c...\n",
      "32561    [ask, scientist, binghamton, university, can, ...\n",
      "34919    [a, new, study, suggests, warm, blooded, anima...\n",
      "Name: message, Length: 35154, dtype: object \n",
      "\n",
      "20267    [rt, redtraccoon, clean, energy, path, cleaner...\n",
      "8723     [rt, miltonwolfmd, nice, try, thehill, the, te...\n",
      "37838    [crayola, common, core, lessons, promote, maoi...\n",
      "8381           [robhunterswords, global, warming, i, tell]\n",
      "19949    [rt, jkuylenstierna, world, leaders, ignore, t...\n",
      "                               ...                        \n",
      "20135    [rt, dbeltwrites, i, know, global, warming, re...\n",
      "22268    [rt, chuckmeg, susanhdon, citizensfedup, funde...\n",
      "12723    [rt, lindasuhler, the, noaa, global, warming, ...\n",
      "10497    [these, graphics, show, terrible, climate, cha...\n",
      "18177    [rt, wsj, in, rebuke, trump, policy, ge, ceo, ...\n",
      "Name: message, Length: 8789, dtype: object \n",
      "\n",
      "Test Set\n",
      "27202    1\n",
      "27644    0\n",
      "28618    0\n",
      "35219    1\n",
      "5766     1\n",
      "        ..\n",
      "22585    2\n",
      "41367    3\n",
      "43394    0\n",
      "32561    1\n",
      "34919    1\n",
      "Name: sentiment, Length: 35154, dtype: int64 \n",
      "\n",
      "20267    1\n",
      "8723     3\n",
      "37838    3\n",
      "8381     0\n",
      "19949    2\n",
      "        ..\n",
      "20135    0\n",
      "22268    1\n",
      "12723    3\n",
      "10497    1\n",
      "18177    2\n",
      "Name: sentiment, Length: 8789, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Split dataset \n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2)\n",
    "\n",
    "print('Train Set')\n",
    "print(x_train, '\\n')\n",
    "print(x_test, '\\n')\n",
    "print('Test Set')\n",
    "print(y_train, '\\n')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74b489e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fct for getting the maximum review length\n",
    "def get_max_length():\n",
    "    review_length = []\n",
    "    for review in x_train:\n",
    "        review_length.append(len(review))\n",
    "\n",
    "    return int(np.ceil(np.mean(review_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3455ce0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded X Train\n",
      " [[    5  3375  2193 ...   136   142   799]\n",
      " [    5 17153 17154 ...     7  1785     0]\n",
      " [    5  6082 11453 ... 17160 17161 17162]\n",
      " ...\n",
      " [    5 63296   273 ...     9    40     0]\n",
      " [  452   167  6441 ...  3007     0     0]\n",
      " [   27    21    95 ...     3     4 63297]] \n",
      "\n",
      "Encoded X Test\n",
      " [[   5 8051  354 ... 3339   24    1]\n",
      " [   5  714  749 ...  327  869  508]\n",
      " [ 961 2880 5325 ...  881 6946 2825]\n",
      " ...\n",
      " [   5 8385   11 ... 8204   61   79]\n",
      " [ 389 6712  293 ...    0    0    0]\n",
      " [   5 1836   65 ...   15    3    4]] \n",
      "\n",
      "Maximum message length:  15\n"
     ]
    }
   ],
   "source": [
    "# ENCODE Message\n",
    "token = Tokenizer(lower=False)    # no need lower, because already lowered the data in load_data()\n",
    "token.fit_on_texts(x_train)\n",
    "x_train = token.texts_to_sequences(x_train)\n",
    "x_test = token.texts_to_sequences(x_test)\n",
    "\n",
    "max_length = get_max_length()\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "total_words = len(token.word_index) + 1   # add 1 because of 0 padding\n",
    "\n",
    "print('Encoded X Train\\n', x_train, '\\n')\n",
    "print('Encoded X Test\\n', x_test, '\\n')\n",
    "print('Maximum message length: ', max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c46871da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 15, 32)            2025536   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                24832     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,050,628\n",
      "Trainable params: 2,050,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ARCHITECTURE\n",
    "EMBED_DIM = 32\n",
    "LSTM_OUT = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, EMBED_DIM, input_length = max_length))\n",
    "model.add(LSTM(LSTM_OUT))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cc54542",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training \n",
    "checkpoint = ModelCheckpoint(\n",
    "    'models/LSTM.h5',\n",
    "    monitor='accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e30c8034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "273/275 [============================>.] - ETA: 0s - loss: 0.9060 - accuracy: 0.6323\n",
      "Epoch 1: accuracy improved from -inf to 0.63239, saving model to models\\LSTM.h5\n",
      "275/275 [==============================] - 7s 21ms/step - loss: 0.9048 - accuracy: 0.6324\n",
      "Epoch 2/5\n",
      "274/275 [============================>.] - ETA: 0s - loss: 0.5155 - accuracy: 0.8017\n",
      "Epoch 2: accuracy improved from 0.63239 to 0.80176, saving model to models\\LSTM.h5\n",
      "275/275 [==============================] - 5s 20ms/step - loss: 0.5153 - accuracy: 0.8018\n",
      "Epoch 3/5\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.2951 - accuracy: 0.8968\n",
      "Epoch 3: accuracy improved from 0.80176 to 0.89680, saving model to models\\LSTM.h5\n",
      "275/275 [==============================] - 5s 20ms/step - loss: 0.2951 - accuracy: 0.8968\n",
      "Epoch 4/5\n",
      "274/275 [============================>.] - ETA: 0s - loss: 0.1624 - accuracy: 0.9457\n",
      "Epoch 4: accuracy improved from 0.89680 to 0.94564, saving model to models\\LSTM.h5\n",
      "275/275 [==============================] - 6s 23ms/step - loss: 0.1625 - accuracy: 0.9456\n",
      "Epoch 5/5\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.0963 - accuracy: 0.9692\n",
      "Epoch 5: accuracy improved from 0.94564 to 0.96922, saving model to models\\LSTM.h5\n",
      "275/275 [==============================] - 6s 22ms/step - loss: 0.0963 - accuracy: 0.9692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x249a1a02fd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size = 128, epochs = 5, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6653468f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275/275 [==============================] - 1s 2ms/step\n",
      "Correct Prediction: 6207\n",
      "Wrong Prediction: 2582\n",
      "Accuracy: 70.62236887017863\n"
     ]
    }
   ],
   "source": [
    "#Testing \n",
    "predict_x = model.predict(x_test) \n",
    "y_pred = np.argmax(predict_x,axis=1)\n",
    "\n",
    "true = 0\n",
    "for i, y in enumerate(y_test):\n",
    "    if y == y_pred[i]:\n",
    "        true += 1\n",
    "\n",
    "print('Correct Prediction: {}'.format(true))\n",
    "print('Wrong Prediction: {}'.format(len(y_pred) - true))\n",
    "print('Accuracy: {}'.format(true/len(y_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f5134c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('models/LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab929003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message : @tiniebeany climate change is an interesting hustle as it was global warming but the planet stopped warming for 15 yes while the suv boom\n"
     ]
    }
   ],
   "source": [
    "message = str(input('Message : '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a624bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned:  @tiniebeany climate change is an interesting hustle as it was global warming but the planet stopped warming for 15 yes while the suv boom\n",
      "Filtered:  ['tiniebeany climate change interesting hustle global warming planet stopped warming  yes suv boom']\n"
     ]
    }
   ],
   "source": [
    "# Pre-process input\n",
    "regex = re.compile(r'[^a-zA-Z\\s]')\n",
    "review = regex.sub('', message)\n",
    "print('Cleaned: ', message)\n",
    "\n",
    "words = review.split(' ')\n",
    "filtered = [w for w in words if w not in english_stops]\n",
    "filtered = ' '.join(filtered)\n",
    "filtered = [filtered.lower()]\n",
    "\n",
    "print('Filtered: ', filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b03f151e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30314     1     2   767 30315     6     7    81  1288     7   274  4586\n",
      "   4277     0     0]]\n"
     ]
    }
   ],
   "source": [
    "tokenize_words = token.texts_to_sequences(filtered)\n",
    "tokenize_words = pad_sequences(tokenize_words, maxlen=max_length, padding='post', truncating='post')\n",
    "print(tokenize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b615a03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 222ms/step\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "result = loaded_model.predict(tokenize_words)\n",
    "y_pred = np.argmax(result,axis=1)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e353aad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
